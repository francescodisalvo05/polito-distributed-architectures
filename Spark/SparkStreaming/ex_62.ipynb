{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-instrument",
   "metadata": {},
   "source": [
    "## Exercise 62 - Distinct + Count\n",
    "\n",
    "Reading : TimeStamp,StockID,Price\n",
    "\n",
    "**Every 30 seconds** print on the standard output the StockID and the price variation (%) in the **last 30 seconds** of the stock with a price variation greater than 0.5% in the last 30 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collected-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the spark object that wait for the \n",
    "# information every two seconds\n",
    "ssc = StreamingContext(sc,30)\n",
    "\n",
    "# the stream will be connected to localhost\n",
    "# through port 9999\n",
    "lineDStream = ssc.SocketTextStream(\"localhost\",9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the stock prices\n",
    "# emit (stockId, (price,price))\n",
    "priceDStreams = lineDStream.map(lambda line : (line.split(\",\")[1], (float(line.split(\",\")[2],float(line.split(\",\")[2]))\n",
    "\n",
    "# the vairation is given by (max(price) - min(price)) / min(price)\n",
    "# so for each stock (key), reduce all the prices\n",
    "minMaxPricesDStreams  = priceDStreams.reduceByKey(lambda r1, r2: (min(r1[0],r2[0]),max(r1[1],r2[1])))\n",
    "\n",
    "# compute the variation as showed before\n",
    "# select the ones with a variation > 0.5%\n",
    "filteredStocks = minMaxPricesDStreams.mapValues(lambda pair : 100.0* ((pair[1] - pair[0]) / pair[0]))\\\n",
    "                                     .filter(lambda variation : variation > 0.5)\n",
    "                                               \n",
    "# print on the standard output\n",
    "filteredStocks.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start reading the incoming data\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay alive for at most 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "# stop only the StreamingContext\n",
    "# but not the SparkContext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
