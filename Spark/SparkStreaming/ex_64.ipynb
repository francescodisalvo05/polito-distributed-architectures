{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passive-westminster",
   "metadata": {},
   "source": [
    "## Exercise 64 - Min/Max + Filter\n",
    "\n",
    "* Textual file (past years) : Timestamp,StockId,Price\n",
    "* Stream of readings : Timestamp,StockId,Price\n",
    "\n",
    "Every 1 minute, by considering only the data received in the last 1 minute, print on the standard output the StockIDs of the stocks that satisfy one of the following conditions: \n",
    "* price of the stock < minimum historical price for that stock\n",
    "* price of the stock > maximum historical price for that stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the static file containing the stations\n",
    "inputStockPrices = \"/data/students/bigdata-01QYD/ex_data/Ex64/data/\"\n",
    "historicalStockPricesRDD = sc.textFile(inputStockPrices)\n",
    "\n",
    "# extract (StockId,(Price,Price)\n",
    "pairsHistoricalStockPricesRDD = historicalStockPricesRDD.map(lambda line : (  line.split(\",\")[1], \n",
    "                                                                              (float(line.split(\",\")[2]),float(line.split(\",\")[2]))  )\n",
    "                                                            ).cache()\n",
    "# compute the minimum and the maximum price for each stock\n",
    "minMaxStockPrices = pairsHistoricalStockPricesRDD.reduceByKey(lambda p1,p2 : (min(p1[0],p2[0]), max(p1[1],p2[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the spark object that wait for the \n",
    "# information every two seconds\n",
    "ssc = StreamingContext(sc,60)\n",
    "\n",
    "# the stream will be connected to localhost\n",
    "# through port 9999\n",
    "lineDStream = ssc.SocketTextStream(\"localhost\",9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit (StockId, (price,price))\n",
    "stocksDStream = lineDStream.map(lambda line: (line.split(\",\")[0], (float(line.split(\",\")[-1]),float(line.split(\",\")[-1])))\n",
    "\n",
    "# compute the local min and max price for each stock because:\n",
    "# - we reduce the number of values to compare\n",
    "# - we don't have duplicates\n",
    "# - we have much less joins\n",
    "minMaxStockPricesDStream = stocksDStream.reduceByKey(lambda p1,p2 : (min(p1[0],p2[0]), max(p1[1],p2[1])))\n",
    "\n",
    "# we need to use transform because we cannot compute the join between an RDD and a DStream\n",
    "# it will return (StockID,(price,(minPrice,maxPrice)))\n",
    "joinedStocksDStream = stocksDStream.transform(lambda batchRDD : batchRDD.join(minMaxStockPrices))\n",
    "\n",
    "# filter the prices according to the request\n",
    "# check if the \"DStream\" min price is < historical min price and vice versa\n",
    "filteredStocksDStream = joinedNamesTimestampDStream.filter(lambda pair : pair[1][0][0] < pair[1][1][0] or \n",
    "                                                                         pair[1][0][0] > pair[1][1][1])\n",
    "\n",
    "# since this condition might be satisfied more than once, we need to use also the distinct()\n",
    "uniqueFilteredStocksDStream = filteredStocksDStream.transform(lambda batchRDD : batchRDD.keys().distinct())\n",
    "                                               \n",
    "# print on the standard output\n",
    "filteredStocksDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start reading the incoming data\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay alive for at most 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "# stop only the StreamingContext\n",
    "# but not the SparkContext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
