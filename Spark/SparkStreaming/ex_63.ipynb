{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "union-tackle",
   "metadata": {},
   "source": [
    "## Exercise 63 - \n",
    "\n",
    "* Textual file : id\\tlongitude\\tlatitude\\tname,\n",
    "* Stream of readings : stationId,#free slots,#used slots,#timestamp \n",
    "\n",
    "For each reading with a number of free slots equal to 0, print on the standard output timestamp and name of the station. Emit new results every 2 seconds by considering only the data received in the last 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the static file containing the stations\n",
    "inputFileStations = \"...\"\n",
    "stationsRDD = sc.textFile(inputFileStations)\n",
    "\n",
    "# we use cache because the join operation will be done for each batch (each pair as you'll see)\n",
    "nameStationsRDD = stationsRDD.map(lambda line : (line.split(\"\\t\")[0], line.split(\"\\t\")[3])).cache()\n",
    "\n",
    "# create the spark object that wait for the \n",
    "# information every two seconds\n",
    "ssc = StreamingContext(sc,2)\n",
    "\n",
    "# the stream will be connected to localhost\n",
    "# through port 9999\n",
    "lineDStream = ssc.SocketTextStream(\"localhost\",9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the readings with #free slots equal to 0\n",
    "# emit (stationId, timestamp)\n",
    "filteredReadingsDStream = lineDStream.filter(lambda line : line.split(\",\")[1] == \"0\")\\\n",
    "                                     .map(lambda line: (line.split(\",\")[0], line.split(\",\")[-1]))\n",
    "\n",
    "# we need to use transform because we cannot compute the join between an RDD and a DStream\n",
    "# it will return (stationId,(timestamp,name))\n",
    "joinedNamesTimestampDStream = filteredReadingsDStream.transform(lambda batchRDD : batchRDD.join(nameStationsRDD))\n",
    "\n",
    "# extract just the value part\n",
    "namesTimestampsDStream = joinedNamesTimestampDStream.map(lambda pair : pair[1])\n",
    "                                               \n",
    "# print on the standard output\n",
    "namesTimestampsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start reading the incoming data\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay alive for at most 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "# stop only the StreamingContext\n",
    "# but not the SparkContext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
