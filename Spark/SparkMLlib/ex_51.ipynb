{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limiting-supervision",
   "metadata": {},
   "source": [
    "## Exercise 51 - Logistic Regression on Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "young-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "removed-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataPath = \"/data/students/bigdata-01QYD/ex_data/Ex51/data/trainingData.csv\"\n",
    "unlabeledDataPath = \"/data/students/bigdata-01QYD/ex_data/Ex51/data/unlabeledData.csv\"\n",
    "outputPath = \"res_ex51/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hidden-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    1|The Spark system ...|\n",
      "|    1|Spark is a new di...|\n",
      "|    0|Turin is a beauti...|\n",
      "|    0|Turin is in the n...|\n",
      "+-----+--------------------+\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "| null|Spark performs be...|\n",
      "| null|Comparison betwee...|\n",
      "| null|Turin is in Piedmont|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData = spark.read.load(trainingDataPath, format=\"csv\", header=True, inferSchema=True)\n",
    "unlabeledData = spark.read.load(unlabeledDataPath, format=\"csv\", header=True, inferSchema=True)\n",
    "trainingData.show()\n",
    "unlabeledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "innocent-yacht",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use two predictors : Presence of word \"Spark\" (bool) and the number of words\n",
    "spark.udf.register(\"checkSpark\", lambda text: \"SPARK\" in text.upper(), BooleanType())\n",
    "spark.udf.register(\"getNumWords\", lambda text: len(text.split(\" \")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "inside-kennedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+--------------------+\n",
      "|label|spark|numWords|                text|\n",
      "+-----+-----+--------+--------------------+\n",
      "|    1| true|       7|The Spark system ...|\n",
      "|    1| true|       6|Spark is a new di...|\n",
      "|    0|false|       5|Turin is a beauti...|\n",
      "|    0|false|       8|Turin is in the n...|\n",
      "+-----+-----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedTrainingData = trainingData.selectExpr(\"label\",\"checkSpark(text) as spark\", \"getNumWords(text) as numWords\", \"text\")\n",
    "updatedTrainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "happy-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- spark: integer (nullable = true)\n",
      " |-- numWords: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedUnlabeledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "documented-democrat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+--------------------+\n",
      "|label|spark|numWords|                text|\n",
      "+-----+-----+--------+--------------------+\n",
      "| null| true|       5|Spark performs be...|\n",
      "| null| true|       5|Comparison betwee...|\n",
      "| null|false|       4|Turin is in Piedmont|\n",
      "+-----+-----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedUnlabeledData = unlabeledData.selectExpr(\"label\",\"checkSpark(text) as spark\", \"getNumWords(text) as numWords\", \"text\")\n",
    "updatedUnlabeledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "indonesian-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the features with the VectorAssembler into a single column called \"features\"\n",
    "assembler = VectorAssembler(inputCols=['spark','numWords'], outputCol='features')\n",
    "\n",
    "# define the model\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.01)\n",
    "\n",
    "# define the pipeline with both assembler and model\n",
    "pipeline = Pipeline().setStages([assembler,lr])\n",
    "classificationModel = pipeline.fit(updatedTrainingData)\n",
    "\n",
    "predictions = classificationModel.transform(updatedUnlabeledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "banner-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+--------------------+---------+--------------------+--------------------+----------+\n",
      "|label|spark|numWords|                text| features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----+--------+--------------------+---------+--------------------+--------------------+----------+\n",
      "| null| true|       5|Spark performs be...|[1.0,5.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
      "| null| true|       5|Comparison betwee...|[1.0,5.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
      "| null|false|       4|Turin is in Piedmont|[0.0,4.0]|[3.19966999960026...|[0.96082185681571...|       0.0|\n",
      "+-----+-----+--------+--------------------+---------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "contemporary-executive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                text|prediction|\n",
      "+--------------------+----------+\n",
      "|Spark performs be...|       1.0|\n",
      "|Comparison betwee...|       1.0|\n",
      "|Turin is in Piedmont|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select just the most relevant information\n",
    "selctedPredictions = predictions.selectExpr(\"text\",\"prediction\")\n",
    "selctedPredictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "presidential-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "selctedPredictions.write.csv(outputPath, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
